{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook is to split the training data set from Kaggle's digit recognizer challenge into a new, smaller training set and a validation set. These new data sets will be used in hyper-parameter tuning and CNN architecture exploration with a view to understanding the optimal CNN architecture for the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data in\n",
    "data = pd.read_csv('datasets/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the label value counts to see how many occurrences of each label exist in the database\n",
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of lists - each sub-list will contain all the indices corresponding to a single label \n",
    "# in the input training set\n",
    "indarray = []\n",
    "for i in range(0,10):\n",
    "    indarray.append(data.index[data.label == i].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the length of the index lists as a check that the data has been divided correctly\n",
    "for i in range(len(indarray)):\n",
    "    print(len(indarray[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and validation subsets of the indarray (index array)\n",
    "trainind = []\n",
    "validationind = []\n",
    "for i in range(len(indarray)):\n",
    "    # calculate '10%'\n",
    "    tenpercent = math.ceil(len(indarray[i]) * 0.1)\n",
    "    # form the validation index and train index arrays\n",
    "    validationind.append(indarray[i][0:tenpercent])\n",
    "    trainind.append(indarray[i][tenpercent:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form the training data set\n",
    "idx = np.concatenate(trainind[:])\n",
    "random.shuffle(idx) # shuffle the indices so that image labels are not grouped in the output set\n",
    "train = data.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.concatenate(validationind[:])\n",
    "random.shuffle(idx) # shuffle the indices so that image labels are not grouped in the output set\n",
    "validation = data.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out all data\n",
    "train.to_csv('datasets/train-exploration.csv', index=False)\n",
    "validation.to_csv('datasets/validation-exploration.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
